{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0342c89",
   "metadata": {
    "id": "b0342c89"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import io, unicodedata, re, string\n",
    "import pandas as pd\n",
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4a60787",
   "metadata": {
    "id": "e4a60787"
   },
   "outputs": [],
   "source": [
    "map_wrong_words = {\n",
    "    'fucksex': 'fuck sex',\n",
    "    'yourselfgo': 'your self ego',\n",
    "    'bcoz': 'because',\n",
    "    'bc': 'because',\n",
    "    'deneid': 'denied',\n",
    "    '\\u200e': '',\n",
    "    'wriminalwar': 'criminal war',\n",
    "    'Pathectic': 'pathetic',\n",
    "    'POLITCAL': 'political',\n",
    "    'talk2me': 'talk to me',\n",
    "    'shitfuck': 'shift fuck',\n",
    "    'BabyWhat': 'baby what',\n",
    "    'Sockpuppetry': 'sock puppetry',\n",
    "    'Bastered': 'bastard',\n",
    "    'PHILIPPINESLONG': 'philippines long',\n",
    "    'SuPeRTR0LL': 'supertroll',\n",
    "    'FUCKBAGS': 'fuck bags',\n",
    "    'peNis': 'penis',\n",
    "    'pensnsnnienSNsn': 'penis',\n",
    "    'pneis': 'penis', \n",
    "    'FooL': 'fool',\n",
    "    'pennnis': 'penis',\n",
    "    'PenIS': 'penis',\n",
    "    'itsuck': 'it suck',\n",
    "    'deletionist': 'delete',\n",
    "    'ReSPeCT': 'respect',\n",
    "    'stuipd' : 'stupid',\n",
    "    '@sshole': 'asshole',\n",
    "    'mf': 'mother fucker',\n",
    "    'b@stard': 'bastard',\n",
    "    's0n': 'son',\n",
    "    'b!tch': 'bitch'\n",
    "}\n",
    "\n",
    "def clean_wrong_spell_words(text, mapping):\n",
    "    for word in mapping:\n",
    "        text = text.replace(word, mapping[word])\n",
    "    return text\n",
    "df['comment_text'] = df['comment_text'].apply(lambda x : clean_wrong_spell_words(x,map_wrong_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1970c8af",
   "metadata": {
    "id": "1970c8af"
   },
   "outputs": [],
   "source": [
    "def cleanHtml(sentence):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, ' ', str(sentence))\n",
    "    return cleantext\n",
    "\n",
    "def cleanPunc(sentence): #function to clean the word of any punctuation or special characters\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    cleaned = cleaned.strip()\n",
    "    cleaned = cleaned.replace(\"\\n\",\" \")\n",
    "    return cleaned\n",
    "\n",
    "def keepAlpha(sentence):\n",
    "    alpha_sent = \"\"\n",
    "    for word in sentence.split():\n",
    "        alpha_word = re.sub('[^a-z A-Z]+', ' ', word)\n",
    "        alpha_sent += alpha_word\n",
    "        alpha_sent += \" \"\n",
    "    alpha_sent = alpha_sent.strip()\n",
    "    return alpha_sent\n",
    "\n",
    "df['comment_text'] = df['comment_text'].apply(lambda x: cleanHtml(x))\n",
    "df['comment_text'] = df['comment_text'].apply(lambda x: cleanPunc(x))\n",
    "df['comment_text'] = df['comment_text'].apply(lambda x: keepAlpha(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9750541a",
   "metadata": {
    "id": "9750541a"
   },
   "outputs": [],
   "source": [
    "def remove_special(cleaned):\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',cleaned)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    cleaned = cleaned.strip()\n",
    "    cleaned = cleaned.replace(\"\\n\",\" \")\n",
    "    cleaned = cleaned.lower()\n",
    "    return cleaned\n",
    "df['comment_text'] = df['comment_text'].apply(lambda x: remove_special(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cf3345e",
   "metadata": {
    "id": "4cf3345e"
   },
   "outputs": [],
   "source": [
    "contraction_mapping = {\n",
    "    \"ain't\": \"is not\", \n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\", \n",
    "    \"'cause\": \"because\", \n",
    "    \"could've\": \"could have\", \n",
    "    \"couldn't\": \"could not\", \n",
    "    \"didn't\": \"did not\",  \n",
    "    \"doesn't\": \"does not\", \n",
    "    \"don't\": \"do not\", \n",
    "    \"hadn't\": \"had not\", \n",
    "    \"hasn't\": \"has not\", \n",
    "    \"haven't\": \"have not\", \n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'll\": \"he will\", \n",
    "    \"he's\": \"he is\", \n",
    "    \"how'd\": \"how did\", \n",
    "    \"how'd'y\": \"how do you\", \n",
    "    \"how'll\": \"how will\", \n",
    "    \"how's\": \"how is\",  \n",
    "    \"I'd\": \"I would\", \n",
    "    \"I'd've\": \"I would have\", \n",
    "    \"I'll\": \"I will\", \n",
    "    \"I'll've\": \"I will have\",\n",
    "    \"I'm\": \"I am\", \n",
    "    \"I've\": \"I have\", \n",
    "    \"i'd\": \"i would\", \n",
    "    \"i'd've\": \"i would have\", \n",
    "    \"i'll\": \"i will\",  \n",
    "    \"i'll've\": \"i will have\",\n",
    "    \"i'm\": \"i am\", \n",
    "    \"i've\": \"i have\", \n",
    "    \"isn't\": \"is not\", \n",
    "    \"it'd\": \"it would\", \n",
    "    \"it'd've\": \"it would have\", \n",
    "    \"it'll\": \"it will\", \"it'll've\": \n",
    "    \"it will have\",\"it's\": \"it is\", \n",
    "    \"let's\": \"let us\", \n",
    "    \"ma'am\": \"madam\", \n",
    "    \"mayn't\": \"may not\", \n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\", \n",
    "    \"must've\": \"must have\", \n",
    "    \"mustn't\": \"must not\", \n",
    "    \"mustn't've\": \"must not have\", \n",
    "    \"needn't\": \"need not\", \n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\", \n",
    "    \"oughtn't\": \"ought not\", \n",
    "    \"oughtn't've\": \"ought not have\", \n",
    "    \"shan't\": \"shall not\", \n",
    "    \"sha'n't\": \"shall not\", \n",
    "    \"shan't've\": \"shall not have\", \n",
    "    \"she'd\": \"she would\", \n",
    "    \"she'd've\": \"she would have\", \n",
    "    \"she'll\": \"she will\", \n",
    "    \"she'll've\": \"she will have\", \n",
    "    \"she's\": \"she is\", \n",
    "    \"should've\": \"should have\", \n",
    "    \"shouldn't\": \"should not\", \n",
    "    \"shouldn't've\": \"should not have\", \n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so as\", \n",
    "    \"this's\": \"this is\",\n",
    "    \"that'd\": \"that would\", \n",
    "    \"that'd've\": \"that would have\", \n",
    "    \"that's\": \"that is\", \n",
    "    \"there'd\": \"there would\", \n",
    "    \"there'd've\": \"there would have\", \n",
    "    \"there's\": \"there is\", \n",
    "    \"here's\": \"here is\",\n",
    "    \"they'd\": \"they would\", \n",
    "    \"they'd've\": \"they would have\", \n",
    "    \"they'll\": \"they will\", \n",
    "    \"they'll've\": \"they will have\", \n",
    "    \"they're\": \"they are\", \n",
    "    \"they've\": \"they have\", \n",
    "    \"to've\": \"to have\", \n",
    "    \"wasn't\": \"was not\", \n",
    "    \"we'd\": \"we would\", \n",
    "    \"we'd've\": \"we would have\", \n",
    "    \"we'll\": \"we will\", \n",
    "    \"we'll've\": \"we will have\", \n",
    "    \"we're\": \"we are\", \n",
    "    \"we've\": \"we have\", \n",
    "    \"weren't\": \"were not\", \n",
    "    \"what'll\": \"what will\", \n",
    "    \"what'll've\": \"what will have\", \n",
    "    \"what're\": \"what are\",  \n",
    "    \"what's\": \"what is\", \n",
    "    \"what've\": \"what have\", \n",
    "    \"when's\": \"when is\", \n",
    "    \"when've\": \"when have\", \n",
    "    \"where'd\": \"where did\", \n",
    "    \"where's\": \"where is\", \n",
    "    \"where've\": \"where have\", \n",
    "    \"who'll\": \"who will\", \n",
    "    \"who'll've\": \"who will have\", \n",
    "    \"who's\": \"who is\", \n",
    "    \"who've\": \"who have\", \n",
    "    \"why's\": \"why is\", \n",
    "    \"why've\": \"why have\", \n",
    "    \"will've\": \"will have\", \n",
    "    \"won't\": \"will not\", \n",
    "    \"won't've\": \"will not have\", \n",
    "    \"would've\": \"would have\", \n",
    "    \"wouldn't\": \"would not\", \n",
    "    \"wouldn't've\": \"would not have\", \n",
    "    \"y'all\": \"you all\", \n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you would\", \n",
    "    \"you'd've\": \"you would have\", \n",
    "    \"you'll\": \"you will\", \n",
    "    \"you'll've\": \"you will have\", \n",
    "    \"you're\": \"you are\", \n",
    "    \"you've\": \"you have\" }\n",
    "def remove_contractions(text, mapping):\n",
    "    for word in mapping:\n",
    "        text = text.replace(word, mapping[word])\n",
    "    return text\n",
    "df['comment_text'] = df['comment_text'].apply(lambda x: remove_contractions(x,contraction_mapping))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a3a8d53",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9a3a8d53",
    "outputId": "69c59501-44aa-4bbb-ed15-cb7366a74e00"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    explanation edits made username hardcore metal...\n",
       "1    daww matches background colour im seemingly st...\n",
       "2    hey man im really trying edit war guy constant...\n",
       "3    cant make real suggestions improvement wondere...\n",
       "4                  sir hero chance remember page thats\n",
       "5             congratulations well use tools well talk\n",
       "6                          cocksucker piss around work\n",
       "7    vandalism matt shirvington article reverted pl...\n",
       "8    sorry word nonsense offensive anyway im intend...\n",
       "9                 alignment subject contrary dulithgow\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "def remove_stopwords(text):\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    cleaned_text = \"\"\n",
    "    for word in text.split():\n",
    "        if word not in stop_words:\n",
    "            cleaned_text += word + ' '\n",
    "    return cleaned_text.strip() \n",
    "df['comment_text'] = df['comment_text'].apply(lambda x: remove_stopwords(x))\n",
    "df['comment_text'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "mkLXjrhEkPge",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mkLXjrhEkPge",
    "outputId": "b1330b7b-2603-426d-a19e-55b47d92e475"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    explanation edits made username hardcore metal...\n",
       "1    daww matches background colour im seemingly st...\n",
       "2    hey man im really trying edit war guy constant...\n",
       "3    cant make real suggestions improvement wondere...\n",
       "4                  sir hero chance remember page thats\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['comment_text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c897ce5e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c897ce5e",
    "outputId": "40861477-f847-4253-c3b4-6b63286486da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         explanation edits made username hardcore metal...\n",
      "1         daww matches background colour im seemingly st...\n",
      "2         hey man im really trying edit war guy constant...\n",
      "3         cant make real suggestions improvement wondere...\n",
      "4                       sir hero chance remember page thats\n",
      "                                ...                        \n",
      "159566    second time asking view completely contradicts...\n",
      "159567                 ashamed horrible thing put talk page\n",
      "159568    spitzer umm theres actual article prostitution...\n",
      "159569    looks like actually put speedy first version d...\n",
      "159570    really dont think understand came idea bad rig...\n",
      "Name: comment_text, Length: 159571, dtype: object\n"
     ]
    }
   ],
   "source": [
    "x =  df.comment_text\n",
    "y =  df.drop(['id','comment_text'],axis = 1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "483c6cb6",
   "metadata": {
    "id": "483c6cb6"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test, y_train,y_test= train_test_split(x,y,test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9149b4d",
   "metadata": {
    "id": "a9149b4d"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vec = TfidfVectorizer(ngram_range=(1,3), min_df=3, max_df=0.9, strip_accents='unicode', use_idf= True, smooth_idf=True, sublinear_tf= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87cbc141",
   "metadata": {
    "id": "87cbc141"
   },
   "outputs": [],
   "source": [
    "transformer = vec.fit(x_train)\n",
    "x_train_transformed = transformer.transform(x_train)\n",
    "x_test_transformed = transformer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ae9b2ab",
   "metadata": {
    "id": "1ae9b2ab"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score , accuracy_score , confusion_matrix , f1_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "logreg = LogisticRegression(C = 10, penalty='l2', solver = 'liblinear', random_state=0)\n",
    "\n",
    "classifier = OneVsRestClassifier(logreg)\n",
    "classifier.fit(x_train_transformed, y_train)\n",
    "\n",
    "\n",
    "#y_train_pred_proba = classifier.predict_proba(x_train_transformed)\n",
    "y_pred = classifier.predict_proba(x_test_transformed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "357e2188",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "357e2188",
    "outputId": "2999e437-fc92-46a8-8195-1659df94e62e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9756306828605658\n"
     ]
    }
   ],
   "source": [
    "roc_auc_score_test = roc_auc_score(y_test, y_pred,average='weighted')\n",
    "print(roc_auc_score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "WzLpJkqvuui-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WzLpJkqvuui-",
    "outputId": "36042fb9-a2f3-4c4e-a238-63cabc28e92a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
      "[[1 0 1 0 1 0]]\n",
      "[[1 1 1 0 1 0]]\n",
      "[[0 0 0 0 0 0]]\n",
      "1\n",
      "[[0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "text=['Rakesh is an asshole']\n",
    "text_transformed = transformer.transform(text)\n",
    "categories = ['toxic','severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "print(categories)\n",
    "print(classifier.predict(text_transformed))\n",
    "\n",
    "text2=['you are a selfish piece of shit, you cock sucking motherfucker']\n",
    "text2_transformed = transformer.transform(text2)\n",
    "print(classifier.predict(text2_transformed))\n",
    "\n",
    "text3=['i am very sorry to disturb you']\n",
    "text3_transformed = transformer.transform(text3)\n",
    "print(classifier.predict(text3_transformed))\n",
    "\n",
    "text4=['Screw it in nice and slow']\n",
    "text4_transformed = transformer.transform(text4)\n",
    "ans=(classifier.predict(text4_transformed))\n",
    "print(ans[0][0])\n",
    "\n",
    "text5 = ['pass me the screw driver']\n",
    "text5_transformed = transformer.transform(text5)\n",
    "print(classifier.predict(text5_transformed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fO0BDod433G8",
   "metadata": {
    "id": "fO0BDod433G8"
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "test['comment_text'] = test['comment_text'].apply(lambda x: clean_wrong_spell_words(x,map_wrong_words))\n",
    "test['comment_text'] = test['comment_text'].apply(lambda x: cleanHtml(x))\n",
    "test['comment_text'] = test['comment_text'].apply(lambda x: cleanPunc(x))\n",
    "test['comment_text'] = test['comment_text'].apply(lambda x: keepAlpha(x))\n",
    "test['comment_text'] = test['comment_text'].apply(lambda x: remove_special(x))\n",
    "test['comment_text'] = test['comment_text'].apply(lambda x: remove_contractions(x,contraction_mapping))\n",
    "test['comment_text'] = test['comment_text'].apply(lambda x: remove_stopwords(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "B6wcJUIr5zNG",
   "metadata": {
    "id": "B6wcJUIr5zNG"
   },
   "outputs": [],
   "source": [
    "test_x =  test.comment_text\n",
    "test_x_transformed = transformer.transform(test_x)\n",
    "test_y = classifier.predict_proba(test_x_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dSDNOpAk6dDz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "dSDNOpAk6dDz",
    "outputId": "7a69b50b-bd93-4150-931e-226735d811b4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>0.999925</td>\n",
       "      <td>0.118802</td>\n",
       "      <td>0.998282</td>\n",
       "      <td>0.027471</td>\n",
       "      <td>0.942335</td>\n",
       "      <td>0.180827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>0.000601</td>\n",
       "      <td>0.000802</td>\n",
       "      <td>0.000690</td>\n",
       "      <td>0.000773</td>\n",
       "      <td>0.003150</td>\n",
       "      <td>0.000689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>0.011476</td>\n",
       "      <td>0.001125</td>\n",
       "      <td>0.004155</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>0.006115</td>\n",
       "      <td>0.001350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.001736</td>\n",
       "      <td>0.001852</td>\n",
       "      <td>0.000376</td>\n",
       "      <td>0.001027</td>\n",
       "      <td>0.000226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>0.017419</td>\n",
       "      <td>0.001599</td>\n",
       "      <td>0.002681</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>0.011314</td>\n",
       "      <td>0.001510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     toxic  severe_toxic   obscene    threat    insult  \\\n",
       "0  00001cee341fdb12  0.999925      0.118802  0.998282  0.027471  0.942335   \n",
       "1  0000247867823ef7  0.000601      0.000802  0.000690  0.000773  0.003150   \n",
       "2  00013b17ad220c46  0.011476      0.001125  0.004155  0.000429  0.006115   \n",
       "3  00017563c3f7919a  0.003397      0.001736  0.001852  0.000376  0.001027   \n",
       "4  00017695ad8997eb  0.017419      0.001599  0.002681  0.000794  0.011314   \n",
       "\n",
       "   identity_hate  \n",
       "0       0.180827  \n",
       "1       0.000689  \n",
       "2       0.001350  \n",
       "3       0.000226  \n",
       "4       0.001510  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'toxic': test_y[:, 0],\n",
    "    'severe_toxic': test_y[:, 1],\n",
    "    'obscene': test_y[:, 2],\n",
    "    'threat': test_y[:, 3],\n",
    "    'insult': test_y[:, 4],\n",
    "    'identity_hate': test_y[:, 5]\n",
    "})\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9wvRu5Kx65jL",
   "metadata": {
    "id": "9wvRu5Kx65jL"
   },
   "outputs": [],
   "source": [
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "OVR.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
